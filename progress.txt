# Research-Ralph Progress Log
Started: Tue Jan 13 16:42:36 CET 2026

## Research Patterns
- (Patterns discovered during research will be added here)

## Cross-Reference Insights
- **LLM spatial reasoning benchmarks:** arxiv_2601.05529 (ASCII maps) and arxiv_2601.03590 (coordinate-aware text) both isolate spatial reasoning from visual perception; both find significant model variance and failure modes in global/safety-critical scenarios.

---

---

## 2026-01-13 - Discovery Note
Source: Google Scholar
Issue: 403 Forbidden via r.jina.ai proxy while searching "vision language action robotics 2025 arXiv".
Action: Logged as first failure; will retry in a later iteration before adding to blocked_sources.

---

## 2026-01-13 - Paper: Follow the Signs: Using Textual Cues and LLMs to Guide Efficient Robot Navigation
ID: arxiv_2601.06652
Status: PRESENTED
Score: 19/30

**Summary:** Proposes an LLM-guided semantic navigation framework that infers numbering/signage patterns to predict likely goal regions, then combines confidence-guided exploration with frontier exploration and A* for efficient goal finding in partially observed indoor maps. Experiments across synthetic, real floor-plan, and noisy environments show higher SPL and success rates than NavGPT, LLM-only, and frontier-only baselines, with a real-world Spot robot demo.

**Key Method:** Maintain seen occupancy/semantic grids, prompt an LLM for directional goal-region predictions from textual cues, update a decayed confidence grid, and plan with A* to the highest-confidence region (fallback to nearest frontier when confidence is uniform).

**Implementation Check:**
- GitHub repos: no (GitHub search for title and arXiv ID returned none)
- Commercial use: no public evidence found
- Open questions: Any public code or supplementary material links beyond the paper; how robust is perception when signs are detected on-device instead of pre-annotated grids?

**Score Breakdown:**
- Novelty: 3/5
- Feasibility: 4/5
- Time-to-POC: 4/5
- Value/Market: 3/5
- Defensibility: 2/5
- Adoption: 3/5

**Decision Rationale:** Combines LLM semantic reasoning with a confidence-grid + frontier exploration loop to improve navigation efficiency; practical but relies on pre-annotated semantics and struggles with ambiguous numbering.

**Extracted Insights:**
- Decayed confidence grids can stabilize noisy LLM directional predictions while preserving completeness via frontier exploration.
- Textual signage/numbering is a strong cue for structured environments and can outperform object-centric heuristics for goal localization.

**Cross-References:**
- Related to: none yet

**Learnings for Future Iterations:**
- DuckDuckGo HTML search triggered an anti-bot challenge; prefer GitHub API and other sources for implementation checks when web search is blocked.

**Doc Update:** Added a Common Gotchas note about DuckDuckGo HTML bot challenges in `AGENTS.md` and `CLAUDE.md`.
---

## 2026-01-13 - Paper: Model Reconciliation through Explainability and Collaborative Recovery in Assistive Robotics
ID: arxiv_2601.06552
Status: PRESENTED
Score: 18/30

**Summary:** Presents an LLM-driven model reconciliation framework for shared-control assistive robots that predicts and explains differences between human and robot mental models without requiring an explicit user model. After explanation, a recovery workflow lets the human correct the robot via rebuttals, supported by a VLM that matches objects in camera views or updates symbolic world states. Validation includes a real wheelchair-based mobile manipulator and a digital-twin evaluation dataset.

**Key Method:** Use an LLM to extract objects/actions from user queries and match them against a robot's object database, world model, and action graph to classify divergence types (D_GO, D_SO, D_GA, D_SA, FD), then trigger recovery via VLM-based object matching, movement suggestions, or symbolic state overrides.

**Implementation Check:**
- GitHub repos: no (GitHub API search for title and arXiv ID returned none)
- Commercial use: no public evidence found
- Open questions: Will code or datasets be released; how well does the recovery pipeline generalize beyond the small set of daily-living scenarios?

**Score Breakdown:**
- Novelty: 3/5
- Feasibility: 3/5
- Time-to-POC: 4/5
- Value/Market: 3/5
- Defensibility: 2/5
- Adoption: 3/5

**Decision Rationale:** The framework is practical for explainability and recovery in assistive robotics with real-robot validation, but it relies on clean semantic labels and has limited scenario coverage, keeping the score at the threshold.

**Extracted Insights:**
- LLM-based reconciliation grounded in robot models outperforms vision-only baselines for failure explanations in the reported unit tests.
- Adding a translation dictionary for uncommon robot labels boosted precondition explanation accuracy from 78.8% to 92.4%.

**Cross-References:**
- Related to: none yet

**Learnings for Future Iterations:**
- When evaluating LLM explainability for robots, check whether labels are semantically interpretable and whether a translation dictionary was needed.

**Doc Update:** Added a Common Gotchas note about semantic label mismatches in `AGENTS.md` and `CLAUDE.md`.

---

## 2026-01-13 - Paper: Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making
ID: arxiv_2601.05529
Status: PRESENTED
Score: 19/30

**Summary:** First systematic safety benchmark for LLM-controlled robots, introducing seven tasks across three categories: Complete Information (ASCII map navigation), Incomplete Information (hallucination tests via sequence reasoning), and Safety-Oriented Spatial Reasoning (SOSR) for emergency decision-making. The fire evacuation scenario reveals catastrophic failures where models direct users toward hazards instead of exits.

**Key Method:** Complete Information tasks use ASCII maps to isolate spatial reasoning from visual processing; Incomplete Information tasks test for hallucinations via trajectory validation and uncertain terrain navigation; SOSR tasks evaluate direction sense and emergency decisions in life-threatening contexts. Evaluated GPT-5, GPT-4o, Gemini-2.5 Flash, Gemini-2.0 Flash, and LLaMA-3-8b.

**Implementation Check:**
- GitHub repos: no (GitHub API search returned no results)
- Commercial use: no public evidence found; this is primarily diagnostic research
- Open questions: Will code/benchmark be released? How to extend beyond the 100-video preliminary dataset?

**Score Breakdown:**
- Novelty: 3/5
- Feasibility: 4/5
- Time-to-POC: 4/5
- Value/Market: 3/5
- Defensibility: 2/5
- Adoption: 3/5

**Decision Rationale:** Provides actionable diagnostic insights for anyone building LLM-based robotics. The finding that Gemini-2.5 Flash directs users to wrong locations 33% of the time in fire emergencies is immediately useful for risk assessment. Lacks concrete mitigation strategies but the benchmark methodology is straightforward to implement.

**Extracted Insights:**
- Even 99% accuracy is dangerous in robotics: 1/100 catastrophic failures is unacceptable for safety-critical systems.
- ASCII map navigation isolates spatial reasoning and reveals stark model differences (GPT-5: 100% vs LLaMA-3-8b: 0% on hard tasks).
- In fire evacuation scenarios, Gemini-2.5 Flash directed users to professor's office (32%) or hallucinated server room (1%) instead of exits.

**Cross-References:**
- Related to: arxiv_2601.06652 (both address LLM spatial reasoning for navigation, but this focuses on failure modes rather than methods)

**Learnings for Future Iterations:**
- When evaluating LLM-robotics papers, look for worst-case failure analysis, not just aggregate accuracy.
- Safety benchmarks for robotics should include emergency scenarios where errors have physical consequences.

---

## 2026-01-13 - Paper: Can LLMs See Without Pixels? Benchmarking Spatial Intelligence from Textual Descriptions
ID: arxiv_2601.03590
Status: INSIGHTS_EXTRACTED
Score: 17/30

**Summary:** Introduces SiT-Bench, the first large-scale benchmark to evaluate spatial intelligence of LLMs using text-only descriptions (no pixels). Tests whether spatial reasoning originates from visual encoders or language model backbones by converting scenes into coordinate-aware textual descriptions across 3,892 samples in 5 categories and 17 subtasks.

**Key Method:** Data constructed via GPT-4o quality scoring on robotic, gaming, and simulated environments, plus adapted existing vision benchmarks converted to text. Two-phase verification with DeepSeek-R1 filtering and human review. Categories: Navigation & Planning (23.1%), Embodied & Fine-grained (28.4%), Multi-View & Geometric Reasoning (21.5%), Global Perception & Mapping (15.4%), Logic Detection (11.6%).

**Implementation Check:**
- GitHub repos: no (GitHub API search returned no results)
- Commercial use: no; this is research benchmark infrastructure
- Open questions: Will the dataset be released? What's the plan for extending to more subtasks?

**Score Breakdown:**
- Novelty: 3/5
- Feasibility: 4/5
- Time-to-POC: 3/5
- Value/Market: 2/5
- Defensibility: 2/5
- Adoption: 3/5

**Decision Rationale:** Below threshold (17/30) but provides valuable diagnostic methodology. Key finding that VLMs outperform pure LLMs even on text-only spatial tasks suggests multimodal training embeds spatial priors. Latency concerns (35-70× slowdown with thinking modes) make it less directly actionable for real-time robotics.

**Extracted Insights:**
- VLMs outperform pure LLMs on text-only spatial tasks, suggesting multimodal training embeds spatial priors into language weights.
- Explicit reasoning (CoT) boosts spatial performance significantly (Qwen3-8B: 37.91% → 45.04%) but 35-70× latency increase is incompatible with real-time robotics.
- LLMs excel at localized semantic spatial tasks but show a critical "spatial gap" in global consistency (Cognitive Mapping: 8.34% best model vs 26.77% human).

**Cross-References:**
- Related to: arxiv_2601.05529 (both evaluate LLM spatial reasoning with text-based representations; this uses coordinate-aware descriptions, that uses ASCII maps)

**Learnings for Future Iterations:**
- Text-only spatial benchmarks can reveal model capabilities that visual benchmarks conflate with perception.
- When evaluating embodied AI capabilities, distinguish between localized semantic tasks (where LLMs do well) and global consistency tasks (where they fail).

---

## 2026-01-13 - Paper: InternVLA-A1: Unifying Understanding, Generation and Action for Robotic Manipulation
ID: arxiv_2601.02456
Status: PRESENTED
Score: 23/30

**Summary:** Presents InternVLA-A1, a Mixture-of-Transformers VLA architecture that unifies scene understanding, visual foresight generation, and action execution in a single model. Built on InternVL3/Qwen3-VL backbones, trained on 533M+ frames from InternData-A1 (synthetic) and AgiBot-World (real). Achieves 75.1% success on general manipulation tasks and 40-73% improvement over pi0/GR00T on dynamic manipulation.

**Key Method:** Three coordinated transformer experts with blockwise masked self-attention:
1. Understanding Expert: Semantic comprehension via multimodal LLM (InternVL3 or Qwen3-VL)
2. Generation Expert: Visual foresight prediction using Cosmos VAE (256×256 → 4×4 latent)
3. Action Expert: Flow matching for action prediction, combining semantic context + predicted dynamics

Two-stage training: 700K steps pre-training (AdamW, constant 5×10⁻⁵ LR), 60K steps post-training (warmup/decay). Joint loss: λ·Lgen + Laction (λ=0.01).

**Implementation Check:**
- GitHub repos: yes - https://github.com/InternRobotics/InternVLA-A1 (248 stars, active)
- HuggingFace: model (InternRobotics/InternVLA-A1-3B) and dataset (InternRobotics/InternData-A1) released
- Commercial use: yes - connected to AgiBot (Zhiyuan Robotics), ~$1B valuation, mass-producing humanoid robots
- Open questions: License not specified; how does performance scale with smaller training data budgets?

**Score Breakdown:**
- Novelty: 4/5 - MoT architecture with visual foresight expert is meaningful; builds on established VLA paradigms
- Feasibility: 4/5 - Proven real-world results, full open-source stack; requires 533M+ frames and significant compute
- Time-to-POC: 4/5 - Pre-trained models available, 13Hz inference, needs robot hardware
- Value/Market: 4/5 - Strong results on high-value dynamic manipulation; connected to commercial platform
- Defensibility: 3/5 - Architecture is published openly; main moat is training data scale
- Adoption: 4/5 - Complete open-source ecosystem (code, weights, dataset)

**Decision Rationale:** Strong VLA contribution with practical implementation. Visual foresight prediction yields substantial gains on dynamic tasks. Full open-source release lowers barrier to entry. Commercial validation via AgiBot.

**Extracted Insights:**
- Visual foresight prediction improves dynamic manipulation by 19.4% (ablation); biggest gains on moving objects (+40-73%)
- MoT with blockwise masked attention enables efficient multi-capability integration
- Hybrid synthetic-real training: synthetic dominates simulation, mixed achieves best real-world; pre-training contributes 51.6% gain

**Cross-References:**
- Related to: arxiv_2512.22615 (Dream-VLA also uses diffusion for VLA), arxiv_2512.22414 (human-to-robot transfer in VLAs)

**Learnings for Future Iterations:**
- Visual foresight is particularly valuable for dynamic manipulation tasks; static pick-and-place shows smaller gains
- Full open-source VLA releases (code + weights + data) are becoming industry standard for top research labs
