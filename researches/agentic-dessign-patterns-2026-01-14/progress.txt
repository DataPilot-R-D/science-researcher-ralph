# Research-Ralph Progress Log
Started: śro 14 sty 12:37:38 2026 CET

## Research Patterns
- **Prototype-to-production gap**: MAS prototypes can be delivered in 2 weeks but production maturity requires 3-6x longer stabilization phase
- **Human oversight persistence**: MAS should augment not replace humans in high-stakes/regulated domains
- **Cost efficiency via specialization**: Proper LLM-based agent architecture can achieve 6x cost reduction over regex baselines
- **Framework selection heuristic**: Graph-based (LangGraph) for branching workflows; Role-based (CrewAI) for team collaboration; Conversational (AutoGen) for prototyping
- **Control-flow integrity for security**: Generate plans in trusted state BEFORE ingesting untrusted data - P-t-E pattern provides inherent prompt injection resistance
- **Dual LLM pattern**: Separate privileged (planning) from quarantined (untrusted input) LLMs for architectural isolation
- **Security through constrained capabilities**: General-purpose agents cannot provide meaningful security guarantees - application-specific agents with pattern-based constraints are the path to production security
- **Six security patterns hierarchy**: Action-Selector (max security) → Plan-Then-Execute → Map-Reduce → Dual LLM → Code-Then-Execute → Context-Minimization (simplest)
- **Lazy agent failure mode**: GRPO loss normalization (1/T_i) inadvertently favors shorter trajectories, causing multi-agent collapse to single-agent behavior - mitigate with causal influence measurement + restart mechanisms

## Cross-Reference Insights
- arxiv_2601.03624 and arxiv_2601.03328 both emphasize governance/accountability patterns
- SIE (Single Information Environment) pattern from arxiv_2601.03328 maps to "Agentic Communities" tier in arxiv_2601.03624
- Both papers highlight hallucination risks and need for guardrails in production
- arxiv_2508.10146 framework taxonomy complements three-tier model from arxiv_2601.03624
- A2A vs ANP protocols are complementary: A2A for enterprise task orchestration, ANP for decentralized discovery

---

## 2026-01-14 - DISCOVERY PHASE COMPLETE

**Phase Transition:** DISCOVERY → ANALYSIS

**Papers Discovered:** 20 (target: 20)

**Sources Used:**
- arXiv API (5 queries)
- Google Scholar via WebSearch
- Web search for recent papers

**Papers by Category:**

### Architectural Patterns & Frameworks (6 papers)
1. Architecting Agentic Communities using Design Patterns (arxiv_2601.03624) - Priority 5
2. LLM-Enabled Multi-Agent Systems: Design Patterns & Paradigms (arxiv_2601.03328) - Priority 5
3. Agentic AI Frameworks: Architectures, Protocols, Design Challenges (arxiv_2508.10146) - Priority 5
4. Fundamentals of Building Autonomous LLM Agents (arxiv_2510.09244) - Priority 5
5. The Landscape of Emerging AI Agent Architectures (arxiv_2404.11584) - Priority 4
6. PublicAgent: Multi-Agent Design Principles (arxiv_2511.03023) - Priority 4

### Security & Resilience Patterns (2 papers)
7. Architecting Resilient LLM Agents: Plan-then-Execute (arxiv_2509.08646) - Priority 5
8. Design Patterns for Securing LLM Agents (arxiv_2506.08837) - Priority 4

### Reasoning Patterns (6 papers)
9. Applying Cognitive Design Patterns to General LLM Agents (arxiv_2505.07087) - Priority 5
10. MAR: Multi-Agent Reflexion (arxiv_2512.20845) - Priority 5
11. Model-First Reasoning LLM Agents (arxiv_2512.14474) - Priority 4
12. Adaptive Graph of Thoughts (arxiv_2502.05078) - Priority 4
13. Demystifying Chains, Trees, and Graphs of Thoughts (arxiv_2401.14295) - Priority 4
14. Thoughts without Thinking: CoT in Agentic Pipelines (arxiv_2505.00875) - Priority 3

### Multi-Agent Collaboration (2 papers)
15. Unlocking Multi-Agent LLM Reasoning: Lazy Agents to Deliberation (arxiv_2511.02303) - Priority 4
16. The Lighthouse of Language: Critique-Guided Improvement (arxiv_2503.16024) - Priority 4

### Memory & Context Management (4 papers)
17. MemRec: Collaborative Memory-Augmented Agentic System (arxiv_2601.08816) - Priority 4
18. Agent Contracts: Resource-Bounded Autonomous AI (arxiv_2601.08815) - Priority 4
19. To Retrieve or To Think? Agentic Context Evolution (arxiv_2601.08747) - Priority 4
20. RAGShaper: Eliciting Agentic RAG Skills (arxiv_2601.08699) - Priority 3

**Date Distribution:**
- 2026 (last 7 days): 7 papers
- 2025 (Oct-Dec): 7 papers
- 2025 (May-Sep): 4 papers
- 2024-2025 (Jan-Apr): 2 papers

**Priority Distribution:**
- Priority 5 (highest): 7 papers
- Priority 4: 11 papers
- Priority 3: 2 papers

**Next Steps:**
- Begin ANALYSIS phase
- Start with highest priority papers (Priority 5)
- First paper: arxiv_2601.03624 "Architecting Agentic Communities using Design Patterns"

---

## 2026-01-14 - Paper: Architecting Agentic Communities using Design Patterns
ID: arxiv_2601.03624
Status: PRESENTED
Score: 23/30

**Summary:** Comprehensive design pattern catalogue (46 patterns) for agentic AI systems organized in three tiers: LLM Agents (task-specific automation), Agentic AI (autonomous reasoning), and Agentic Communities (multi-participant coordination). Uniquely grounds patterns in ISO ODP-EL formal semantics enabling verifiable governance and accountability.

**Key Method:** Three-tier classification derived from ISO/IEC 15414 Open Distributed Processing Enterprise Language. Uses deontic tokens (burden/permit/embargo) for formal accountability chains. Three-step design methodology: (1) Assess use case characteristics, (2) Apply pattern composition (vertical/horizontal/cross-cutting), (3) Scope implementation tier.

**Implementation Check:**
- GitHub repos: Yes
  - https://github.com/sarwarbeing-ai/Agentic_Design_Patterns (Antonio Gulli's book implementation)
  - https://github.com/promptadvisers/agentic-design-patterns-docs (21 patterns documented)
  - https://github.com/nibzard/awesome-agentic-patterns (curated pattern catalogue)
  - https://github.com/ksm26/AI-Agentic-Design-Patterns-with-AutoGen (AutoGen implementation)
- Commercial use: No direct commercialization found
- Open questions: How do ODP-EL formal semantics integrate with existing frameworks (LangChain, CrewAI)? Runtime verification tooling not yet available.

**Score Breakdown:**
- Novelty: 4/5 (Three-tier framework + ODP-EL formal grounding is unique)
- Feasibility: 4/5 (46 patterns with concrete guidance; incremental adoption path)
- Time-to-POC: 3/5 (Patterns usable immediately; ODP-EL has learning curve)
- Value/Market: 5/5 (Enterprise need for systematic agent architecture; compliance-ready)
- Defensibility: 3/5 (Patterns are reusable knowledge; formal grounding is differentiated)
- Adoption: 4/5 (Framework-agnostic; works with existing agent frameworks)

**Decision Rationale:** Highest-value paper for research focus. Three-tier framework directly applicable to building production agents. ODP-EL formal grounding distinguishes from narrative pattern catalogues.

**Extracted Insights:**
- Three-tier classification (LLM Agents→Agentic AI→Agentic Communities) provides clear scoping for incremental development
- Deontic tokens (burden/permit/embargo) enable formal accountability - critical for regulated industries
- Pattern composition: Vertical (foundational→sophisticated), Horizontal (peer patterns), Cross-Cutting (governance overlay)
- Intent vs Obligation distinction preserves autonomy while enabling responsibility chains

**Cross-References:**
- Patterns referenced: ReAct (#1), Reflexion (#6), Constitutional AI (#7), Plan-then-Execute (#41), Memory-Augmented (#3)
- Related papers in pool: arxiv_2509.08646 (Plan-then-Execute focus), arxiv_2512.20845 (Multi-Agent Reflexion), arxiv_2505.07087 (Cognitive patterns)

**Learnings for Future Iterations:**
- Papers grounding agent patterns in formal methods (ISO standards, deontic logic) score higher on enterprise applicability
- GitHub implementation search yields related resources even when paper itself has no direct implementation
- Clinical trial case study pattern: domain-specific case studies validate enterprise applicability

---

## 2026-01-14 - Paper: LLM-Enabled Multi-Agent Systems: Empirical Evaluation
ID: arxiv_2601.03328
Status: PRESENTED
Score: 23/30

**Summary:** Empirical evaluation of LLM-enabled multi-agent systems across three real-world domains: telecom security, heritage asset management, and utilities customer service. Formalizes design patterns including ReAct agents, Single Information Environment (SIE), hierarchical architectures, supervisor patterns, and swarm configurations.

**Key Method:** Three controlled containerized pilots with stakeholder feedback, sentiment analysis, and UAT with Likert scales measuring correctness, usefulness, clarity, groundedness, and safety. Measures development velocity, cost efficiency, and throughput quantitatively.

**Implementation Check:**
- GitHub repos: General multi-agent patterns (https://github.com/NisaarAgharia/AI-Agents - 83 stars)
- Google ADK patterns: https://developers.googleblog.com/developers-guide-to-multi-agent-patterns-in-adk/
- Commercial use: Yes - Authors from Kaze Technologies/Kaze Consulting (Bath, UK)
- Open questions: How to bridge prototype-to-production gap? What guardrails work for regulated domains?

**Case Study Results:**
| Case | Domain | Architecture | Key Metric |
|------|--------|--------------|------------|
| CS1 | Telecom Security | SIE | CISO approved further investigation |
| CS2 | National Heritage | SIE | 61% positive sentiment |
| CS3 | Utilities Customer Service | Hierarchical | 100% email categorization, £0.05/email vs £0.33 baseline |

**Score Breakdown:**
- Novelty: 3/5 (Formalizes existing patterns; value is in empirical validation)
- Feasibility: 5/5 (Prototypes in 2 weeks, pilots in 1 month - real case studies)
- Time-to-POC: 5/5 (Demonstrated rapid prototyping; clear architectural guidance)
- Value/Market: 4/5 (Enterprise applicability validated; addresses real business needs)
- Defensibility: 2/5 (Patterns are public knowledge; no proprietary advantage)
- Adoption: 4/5 (Works with LangGraph, existing frameworks)

**Decision Rationale:** Highly practical paper with real empirical data from 3 enterprise domains. Key insight: prototype speed doesn't equal production speed - critical for planning.

**Extracted Insights:**
- Prototype-to-production gap: 2 weeks to prototype, but production maturity requires extensive tuning (plan 3-6x longer)
- Single Information Environment (SIE) pattern: Data-centric design with specialist agents per dataset
- Human oversight remains persistent requirement even in automated domains
- Cost efficiency: 6x cost reduction achievable with proper MAS architecture

**Cross-References:**
- SIE architecture relates to "Agentic Communities" tier from arxiv_2601.03624
- Hierarchical patterns share taxonomy with three-tier framework
- Hallucination/grounding risks echo formal governance needs from arxiv_2601.03624

**Learnings for Future Iterations:**
- Papers with quantitative metrics (£/email, throughput, accuracy) are more actionable than pure theoretical frameworks
- Authors' commercial affiliation indicates real-world applicability
- Google ADK documentation provides complementary pattern catalogue (8 patterns: Sequential Pipeline, Coordinator, Parallel Fan-Out, Hierarchical, Generator-Critic, Iterative Refinement, Human-in-Loop, Composite)

---

## 2026-01-14 - Paper: Agentic AI Frameworks: Architectures, Protocols, and Design Challenges
ID: arxiv_2508.10146
Status: PRESENTED
Score: 23/30

**Summary:** Systematic review and comparative analysis of 7 major agentic AI frameworks (CrewAI, LangGraph, AutoGen, Semantic Kernel, Agno, Google ADK, MetaGPT) and 4 agent communication protocols (MCP, ACP, A2A, ANP). Establishes foundational taxonomy for agentic AI systems covering orchestration approaches, memory implementations, guardrails, and service computing readiness.

**Key Method:** Multi-dimensional framework comparison across: architectural approaches (graph-based vs role-based vs conversational), memory implementations (stateful nodes vs agent-level vs shared dialogue), guardrails (validators, retry logic, trust layers), and SOA alignment. Protocol analysis covers message formats, discovery mechanisms, and transport layers.

**Implementation Check:**
- GitHub repos: Yes - all frameworks analyzed are open-source
  - https://github.com/a2aproject/A2A (A2A Protocol - Linux Foundation/Google)
  - https://github.com/agent-network-protocol/AgentNetworkProtocol (ANP Protocol)
  - https://github.com/langchain-ai/langgraph (LangGraph)
  - https://github.com/crewAIInc/crewAI (CrewAI)
  - https://github.com/microsoft/autogen (AutoGen)
- Commercial use: Yes - all frameworks are backed by commercial entities
  - LangChain/LangGraph: LangChain Inc
  - CrewAI: CrewAI Inc
  - AutoGen & Semantic Kernel: Microsoft
  - Google ADK: Google
- Open questions: How will A2A vs ANP interoperability evolve? When will standardized benchmarks emerge?

**Framework Comparison Summary:**

| Framework | Architecture | Primary Focus | Best Use Case |
|-----------|-------------|---------------|---------------|
| LangGraph | Graph-based | State management & control flow | Complex branching workflows |
| AutoGen | Conversational/Event-driven | Multi-agent conversations | Dialogue & prototyping |
| CrewAI | Role-based teams | Collaborative task execution | Structured team collaboration |
| Semantic Kernel | Modular/Enterprise | Fine-grained planning control | Enterprise orchestration |
| MetaGPT | Role-based (SWE) | Software engineering simulation | Code generation teams |
| Google ADK | Multi-agent workflows | Scale & cloud integration | Cloud-native deployments |
| Agno | Declarative | Transparency | Lightweight agents |

**Protocol Comparison:**

| Protocol | Message Format | Discovery | Key Innovation |
|----------|---------------|-----------|----------------|
| MCP | JSON-RPC | Manual | Structured tool calls via schema validation |
| A2A | JSON-RPC/HTTP | Agent Cards | Memory, goal coordination, capability discovery |
| ANP | JSON-LD + NLP | DID-based | Decentralized identifiers, semantic interop |
| Agora | Meta-layer | Protocol Docs | Multi-protocol coordination |

**Five Critical Design Challenges Identified:**
1. Rigid architectures - static agent roles limit mid-execution adaptation
2. No runtime discovery - peers cannot dynamically locate capabilities
3. Code safety risks - generated code execution poses file system/shell vulnerabilities
4. Interoperability gaps - framework-specific abstractions prevent seamless integration
5. Missing standards - lack of universal agent communication contracts

**Score Breakdown:**
- Novelty: 3/5 (Survey paper synthesizing existing work; novel taxonomy and protocol comparison)
- Feasibility: 5/5 (All frameworks analyzed are production-ready open-source)
- Time-to-POC: 5/5 (Direct framework recommendations enable quick prototyping decisions)
- Value/Market: 4/5 (High enterprise demand for framework selection guidance)
- Defensibility: 2/5 (Comparative knowledge is public; no proprietary advantage)
- Adoption: 4/5 (Framework-agnostic analysis; protocol recommendations actionable)

**Decision Rationale:** Essential reference for framework selection. Five design challenges directly inform production architecture decisions. Protocol comparison (A2A vs ANP) clarifies interoperability strategy.

**Extracted Insights:**
- Framework selection heuristic: Graph-based (LangGraph) for branching; Role-based (CrewAI) for teams; Conversational (AutoGen) for prototyping
- Five critical design challenges map directly to production risks
- A2A vs ANP are complementary not competing: A2A for enterprise task orchestration, ANP for decentralized discovery
- Memory implementations vary significantly: stateful nodes (LangGraph), agent-level (CrewAI), shared dialogue (AutoGen)

**Cross-References:**
- Memory patterns connect to arxiv_2601.08816 (MemRec)
- Guardrails patterns connect to arxiv_2506.08837 (security patterns)
- Framework taxonomy complements three-tier model from arxiv_2601.03624
- Protocol analysis extends SIE pattern discussion from arxiv_2601.03328

**Learnings for Future Iterations:**
- Survey papers score high on feasibility/time-to-POC due to immediate actionability
- Protocol landscape is rapidly evolving (A2A launched June 2025 under Linux Foundation)
- ArXiv HTML version often more accessible than PDF for extraction
- Cross-referencing frameworks to protocols reveals interoperability strategy

---

## 2026-01-14 - Paper: Fundamentals of Building Autonomous LLM Agents
ID: arxiv_2510.09244
Status: PRESENTED
Score: 19/30

**Summary:** TUM seminar technical report defining a four-component architecture for autonomous LLM agents: Perception (environmental sensing), Reasoning (planning and adaptation), Memory (short/long-term storage), and Execution (action translation). Synthesizes existing patterns (ReAct, CoT, ToT, RAG, MCTS) into a coherent educational framework with practical design recommendations.

**Key Method:** Structured review organizing agent capabilities into four cognitive-inspired systems. The paper identifies:
- Four perception approaches: text-based, multimodal, structured data (DOM/accessibility trees), tool-augmented
- Four reasoning patterns: task decomposition (HuggingGPT, ReAct), multi-plan generation (ToT, MCTS), reflection, multi-agent experts
- Three memory types: long-term (RAG, SQL, fine-tuning), short-term (context window), storage (experiences, procedures, domain, user)
- Four execution modes: tool integration, visual/GUI automation, code generation, robotic control

**Implementation Check:**
- GitHub repos: Multiple educational resources found
  - https://github.com/artnitolog/awesome-agent-learning (courses + assignments)
  - https://github.com/HKUDS/AutoAgent (zero-code agent framework)
  - https://github.com/victordibia/designing-multiagent-systems (multi-agent from scratch)
  - https://github.com/tmgthb/Autonomous-Agents (research papers repository)
- Commercial use: No direct commercialization; TUM academic context
- Open questions: How to close 72% vs 43% human-AI performance gap? Solutions for repetitive action loops?

**Score Breakdown:**
- Novelty: 2/5 (Educational synthesis; consolidates known patterns without advancing state-of-art)
- Feasibility: 5/5 (Four-component model directly implementable; clear mental model)
- Time-to-POC: 4/5 (Clear principles; multiple linked implementations accelerate prototyping)
- Value/Market: 3/5 (High educational value; no competitive edge as public knowledge synthesis)
- Defensibility: 1/5 (Seminar report; no proprietary techniques or novel methods)
- Adoption: 4/5 (Framework-agnostic; principles applicable to any agent implementation)

**Decision Rationale:** Meets threshold as foundational educational reference. Four-component framework provides clear mental model for agent design. Benchmark gap analysis (72% human vs 43% AI) quantifies current limitations.

**Extracted Insights:**
- Four-component architecture (Perception→Reasoning→Memory→Execution) provides foundational mental model
- Human-AI performance gap: 72%+ for humans vs ~43% for leading AI on OSWorld - sets realistic expectations
- Agent vs workflow distinction: "Simply augmenting an LLM with modules, tools, or predefined steps does not make it an agent" - agents act according to feedback

**Critical Challenges Identified:**
1. GUI grounding - mapping screenshots to precise interaction coordinates unreliable
2. Repetitive actions - agents frequently enter unproductive loops
3. UI robustness - unexpected elements/layout changes cause failures
4. Context window - constraints require aggressive summarization

**Cross-References:**
- Four-component model relates to three-tier framework from arxiv_2601.03624
- Memory patterns connect to arxiv_2508.10146 framework comparison
- Reasoning patterns (CoT, ToT, MCTS) connect to arxiv_2502.05078 and arxiv_2401.14295

**Learnings for Future Iterations:**
- Educational synthesis papers score lower on novelty/defensibility but high on feasibility/adoption
- Quantified benchmark gaps (72% vs 43%) are valuable for setting expectations
- Agent vs workflow distinction is foundational for avoiding over-engineered solutions

---

## 2026-01-14 - Paper: Architecting Resilient LLM Agents: Secure Plan-then-Execute
ID: arxiv_2509.08646
Status: PRESENTED
Score: 25/30

**Summary:** Security-first guide to Plan-then-Execute (P-t-E) architecture for LLM agents. Separates strategic planning (powerful LLM generates structured plan) from tactical execution (simpler Executor carries out steps). Core innovation is framing P-t-E as inherent defense against indirect prompt injection through control-flow integrity - plans generated in trusted state before ingesting untrusted external data.

**Key Method:** Defense-in-depth strategy with multiple layers:
1. Control-flow integrity - locked execution sequences resist manipulation
2. Principle of least privilege - executors receive only task-scoped tools
3. Dual LLM pattern - separate privileged (planning) and quarantined (untrusted input) LLMs
4. Input sanitization and output filtering (PII, policy violations)
5. Docker containerization mandatory for code execution
6. Human-in-the-loop verification for critical actions

**Implementation Check:**
- GitHub repos: Yes
  - https://github.com/RichardHGL/CHI2025_Plan-then-Execute_LLMAgent (CHI 2025 user study, 248 participants)
  - https://github.com/dasiths/llm-plan-and-execute-knowledge-provider-mesh (Dapr-based distributed implementation)
  - https://github.com/gitcommitshow/resilient-llm (resilient multi-LLM orchestration library)
- Commercial use: Yes - SAP under review as recommended architecture pattern for multi-agent solutions
- Framework blueprints: LangGraph (state machines), CrewAI (hierarchical/declarative), AutoGen (GroupChat + Docker)
- Open questions: How to minimize upfront latency? Optimal re-planning trigger conditions?

**Score Breakdown:**
- Novelty: 4/5 (Security-first framing of P-t-E is novel; control-flow integrity concept unique)
- Feasibility: 5/5 (Framework implementations provided; practical guidance immediately usable)
- Time-to-POC: 4/5 (Code blueprints for 3 frameworks; SAP adoption accelerates enterprise path)
- Value/Market: 5/5 (SAP adoption validates enterprise demand; security focus critical post-CVE-2025-53773)
- Defensibility: 3/5 (Pattern is public knowledge; security integration approach differentiates)
- Adoption: 4/5 (Works with LangGraph, CrewAI, AutoGen; SAP enterprise validation)

**Decision Rationale:** Highest-scoring paper so far (25/30). Security-first framing addresses critical production gap identified in earlier papers. SAP enterprise adoption validates market readiness. CHI 2025 user study provides human factors data on trust calibration.

**Extracted Insights:**
- Control-flow integrity: Plans generated in trusted state BEFORE ingesting untrusted data - prevents prompt injection by design
- Principle of least privilege: Task-scoped tool access prevents unauthorized function calls even under attack
- Dual LLM pattern: Architectural isolation between privileged/quarantined LLMs prevents privilege escalation
- Trade-offs: Better reasoning/security vs upfront latency and wasted effort on early failures

**Cross-References:**
- Complements arxiv_2506.08837 (security design patterns) - both address prompt injection
- Extends Plan-then-Execute (#41) from arxiv_2601.03624 three-tier pattern catalogue
- Framework implementations relate to arxiv_2508.10146 framework comparison
- Human trust calibration finding connects to human oversight requirements from arxiv_2601.03328

**Security Landscape Context:**
- OWASP 2025: Prompt injection is fundamental architectural vulnerability requiring defense-in-depth
- Recent research (NAACL 2025): Adaptive attacks bypass 12 published defenses with >90% success rate
- Microsoft FIDES: Information-flow control techniques for deterministic prompt injection prevention
- GitHub Copilot CVE-2025-53773: Remote code execution (CVSS 9.6) demonstrates real-world risk

**Learnings for Future Iterations:**
- Papers with enterprise adoption (SAP) validation score higher on market value
- Security-focused papers increasingly relevant as agent systems move to production
- CHI user studies provide valuable human factors data beyond technical benchmarks
- Cross-referencing security literature (OWASP, CVEs) provides real-world context

---

## 2026-01-14 - Paper: Applying Cognitive Design Patterns to General LLM Agents
ID: arxiv_2505.07087
Status: PRESENTED
Score: 18/30

**Summary:** Maps cognitive design patterns from pre-transformer AI architectures (Soar, ACT-R, BDI) to modern LLM-based agents. Identifies six key cognitive patterns and their presence/absence in agentic LLM systems, revealing specific gaps that represent research opportunities toward general intelligence.

**Key Method:** Comparative cognitive architecture analysis examining patterns from 40+ years of research. Addresses three questions: (1) Which patterns appear in existing agentic LLM systems? (2) Which seem relevant but remain underexplored? (3) Do LLMs suggest novel patterns?

**Cognitive Patterns Identified:**

| Pattern | Status in LLM Agents | Examples |
|---------|---------------------|----------|
| Observe-Decide-Act | Present (partial) | ReAct (lacks commitment stage) |
| Hierarchical Decomposition | Present | Voyager, Tree of Thoughts |
| Memory Types | Present | Generative Agents (episodic), RAG (semantic) |
| Knowledge Compilation | Emerging | ExpeL, Reflexion, Voyager |
| Commitment & Reconsideration | **UNDEREXPLORED** | Gap - BDI has it, ReAct lacks it |
| Step-wise Reflection | Novel (LLM-specific) | Iterative self-evaluation pattern |

**Implementation Check:**
- GitHub repos: Yes
  - https://github.com/SoarGroup/Soar (Soar cognitive architecture, BSD license)
  - https://github.com/soartech/jsoar (Java implementation)
  - https://github.com/ysymyth/awesome-language-agents (CoALA framework + 70+ agent papers)
- Commercial use: Soar Technology Inc (defense-focused AI); no direct LLM agent product
- Open questions: Would introducing explicit commitment to ReAct improve reasoning? How to implement reconsideration efficiently in LLM agents?

**Score Breakdown:**
- Novelty: 4/5 (Novel bridging of cognitive science + LLM agents; identifies specific gaps)
- Feasibility: 3/5 (Theoretical framework; requires cognitive architecture expertise)
- Time-to-POC: 3/5 (Research-oriented; Soar has learning curve)
- Value/Market: 3/5 (High research value; limited immediate commercial application)
- Defensibility: 2/5 (Patterns are public knowledge synthesis from 40+ year history)
- Adoption: 3/5 (Framework-agnostic; requires understanding BDI/Soar concepts)

**Decision Rationale:** Meets threshold (18/30). Valuable theoretical foundation bridging cognitive science and LLM agents. The commitment/reconsideration gap is a concrete research direction for improving agent reasoning.

**Extracted Insights:**
- Commitment & Reconsideration is the most underexplored cognitive pattern in LLM agents
- Knowledge Compilation becomes strategically important as reasoning model costs rise
- Step-wise Reflection is a novel LLM-specific pattern not found in traditional cognitive architectures

**Cross-References:**
- BDI (Belief-Desire-Intention) architecture relates to three-tier framework from arxiv_2601.03624
- Memory types connect to arxiv_2508.10146 framework memory comparison and arxiv_2601.08816 (MemRec)
- Knowledge compilation (ExpeL, Reflexion) connects to reflection patterns in arxiv_2512.20845 (MAR)
- Four-component model from arxiv_2510.09244 is a simplified version of full cognitive architecture

**Learnings for Future Iterations:**
- Papers bridging established AI fields (cognitive science) with LLM agents provide unique value
- AGI conference papers (AGI25) tend to focus on theoretical frameworks vs practical implementations
- Soar has 40+ years of research history - rich source for pattern identification
- CoALA framework provides good synthesis of cognitive architecture + language agent intersection

---

## 2026-01-14 - Paper: Design Patterns for Securing LLM Agents against Prompt Injections
ID: arxiv_2506.08837
Status: PRESENTED
Score: 26/30

**Summary:** Comprehensive security framework proposing six principled design patterns for building AI agents with provable resistance to prompt injection. Multi-institution collaboration from IBM, Invariant Labs, ETH Zurich, Google, and Microsoft. Core assertion: general-purpose agents cannot provide meaningful security guarantees - application-specific agents with constrained capabilities are the path forward.

**Six Design Patterns:**

| Pattern | Security Mechanism | Trade-off |
|---------|-------------------|-----------|
| Action-Selector | No feedback loops; LLM selects from fixed action menu | Maximum security; minimum flexibility |
| Plan-Then-Execute | Control-flow integrity; plan locked before untrusted data | Good balance; requires predetermined sequences |
| LLM Map-Reduce | Isolated LLM instances per chunk; safe output formats | Parallel safety; task must be decomposable |
| Dual LLM | Privilege separation; quarantined LLM has no tools | Strong isolation; architectural complexity |
| Code-Then-Execute | Explicit formal syntax makes manipulation visible | Requires code generation capability |
| Context-Minimization | User prompt removed before processing results | Simple; limits personalization |

**Implementation Check:**
- GitHub repos: Yes
  - https://pondevelopment.github.io/llm-prompt-injection-mitigation-patterns/ (interactive guide)
  - https://github.com/tldrsec/prompt-injection-defenses (comprehensive defense catalogue)
  - https://github.com/Joe-B-Security/awesome-prompt-injection (curated references)
- Commercial use: Yes
  - Authors from IBM, Google, Microsoft, ETH Zurich
  - Archestra AI offers commercial Dual LLM implementation
  - LLM Guard, NeMo-Guardrails, IBM Granite Guardian implement related patterns
- Open questions: How to evaluate security-utility trade-offs for specific applications? Optimal pattern combinations for different threat models?

**Ten Case Study Applications:**
1. OS Assistant - Malicious filenames/contents executing shell commands
2. SQL Agent - Database-injected queries; Python interpreter exploitation
3. Email/Calendar - Third-party emails/invitations; data exfiltration
4. Customer Service - User prompt injection; screenshot attacks
5. Booking - Third-party content in calendar/service descriptions
6. Product Recommender - User reviews containing ranking manipulation
7. Resume Screening - Resume content discrediting/boosting candidates
8. Medication Chatbot - User prompts requesting off-label statements
9. Medical Diagnosis - Patient prompt injection in diagnosis summaries
10. Software Engineering - Malicious documentation/packages; code injection

**Score Breakdown:**
- Novelty: 5/5 (Provable security properties for each pattern; novel formalization)
- Feasibility: 4/5 (Patterns immediately implementable; architectural discipline required)
- Time-to-POC: 4/5 (Interactive guides available; pattern selection requires threat modeling)
- Value/Market: 5/5 (Critical for production agents; multi-institution validation)
- Defensibility: 4/5 (Patterns public; implementation expertise differentiates)
- Adoption: 4/5 (Framework-agnostic; works with LangGraph, CrewAI, AutoGen)

**Decision Rationale:** Highest-scoring paper (26/30). Authoritative reference from leading AI security researchers. Six patterns form foundational toolkit for production agent security. Key insight: security comes from constrained capabilities, not from making LLMs more robust.

**Extracted Insights:**
- General-purpose agents cannot provide meaningful security guarantees
- Six patterns form hierarchy from max security/min flexibility to simple/moderate protection
- LLM Map-Reduce ideal for batch classification, review aggregation, resume screening
- Context-Minimization simplest pattern to implement for preventing output manipulation

**Cross-References:**
- Directly complements arxiv_2509.08646 (Plan-then-Execute security guide)
- Security patterns integrate with three-tier framework from arxiv_2601.03624
- Dual LLM pattern extends privilege separation from arxiv_2509.08646
- Map-Reduce pattern relates to SIE architecture from arxiv_2601.03328

**Learnings for Future Iterations:**
- Multi-institution papers (IBM, Google, Microsoft, ETH Zurich) indicate high research rigor
- Security patterns are increasingly critical as agents move to production
- Interactive implementation guides (like pondevelopment.github.io) accelerate adoption
- "Provable resistance" claims require careful examination of assumptions and threat models

---

## 2026-01-14 - Paper: The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling
ID: arxiv_2404.11584
Status: PRESENTED
Score: 20/30

**Summary:** Foundational survey (April 2024) establishing taxonomy of AI agent implementations for reasoning, planning, and tool execution. Classifies architectures into single-agent vs multi-agent (vertical/hierarchical vs horizontal/egalitarian), identifies five major planning approaches, and evaluates critical success factors including leadership impact and communication styles.

**Key Method:** Systematic review of agent implementations across both paradigms. Single-agent: ReAct (reasoning-action interleaving), RAISE (memory augmentation), Reflexion (self-reflection via linguistic feedback), LATS (tree-based search). Multi-agent: AgentVerse (four-stage execution), MetaGPT (structured outputs + pub-sub), DyLAN (dynamic team rotation). Evaluates planning-execution-evaluation phases as core success pattern.

**Implementation Check:**
- GitHub repos: Yes
  - https://github.com/luo-junyu/Awesome-Agent-Papers (tracks survey papers including this one)
  - https://github.com/tmgthb/Autonomous-Agents (daily-updated research papers)
  - https://github.com/AGI-Edgerunners/LLM-Agents-Papers (paper + code links)
  - BabyAGI (https://github.com/yoheinakajima/babyagi) referenced as example
- Commercial use: No direct commercialization; influenced framework designs
- Open questions: How to build standardized eval benchmarks? Dynamic evaluation resistant to memorization?

**Five Planning Approaches Identified:**
1. Task decomposition - Break tasks into subtasks
2. Multi-plan selection - Generate and select from multiple plans
3. External module-aided planning - Leverage external planners
4. Reflection and refinement - Revise based on feedback
5. Memory-augmented planning - Incorporate past experiences

**Key Findings:**
- Agent teams with organized leader complete tasks ~10% faster (with human oversight)
- Multi-agent discussion doesn't necessarily enhance reasoning when single-agent prompts are robust
- Single-agent best for well-defined problems; multi-agent for collaborative/parallel needs
- Major eval limitations: inconsistent benchmarks, data contamination, limited real-world applicability

**Score Breakdown:**
- Novelty: 2/5 (Survey synthesis; published April 2024, somewhat dated)
- Feasibility: 5/5 (Clear taxonomy immediately usable for architecture decisions)
- Time-to-POC: 4/5 (Framework recommendations enable quick prototyping)
- Value/Market: 4/5 (Well-cited 252+; taxonomy still referenced in 2025-2026 papers)
- Defensibility: 1/5 (Public knowledge synthesis; no proprietary advantage)
- Adoption: 4/5 (Framework-agnostic; principles broadly applicable)

**Decision Rationale:** Meets threshold as foundational reference. Taxonomy of single vs multi-agent and five planning approaches remain authoritative. Benchmark/eval criticisms prescient and still being addressed by community.

**Extracted Insights:**
- Single vs multi-agent selection based on task characteristics, not reasoning complexity
- Leadership in multi-agent systems provides measurable efficiency gains
- Five planning approaches form foundation for structured agent reasoning
- Over-engineering warning: multi-agent not always better than robust single-agent

**Cross-References:**
- Taxonomy aligns with arxiv_2508.10146 framework comparison (2025)
- Planning patterns relate to arxiv_2601.03624 three-tier model
- Single/multi-agent decision guidance complements arxiv_2601.03328 empirical findings
- Leadership pattern connects to hierarchical patterns in arxiv_2601.03624

**Learnings for Future Iterations:**
- Foundational surveys remain valuable even after 20+ months - core concepts endure
- Well-cited papers (252+) indicate lasting community influence
- Eval/benchmark criticisms are recurring themes across agent literature
- 2024 landscape has evolved significantly but taxonomies remain stable

---

## 2026-01-14 - Paper: PublicAgent: Multi-Agent Design Principles From an LLM-Based Open Data Analysis Framework
ID: arxiv_2511.03023
Status: PRESENTED
Score: 22/30

**Summary:** Multi-agent framework for open data analysis deriving five empirically-validated design principles from evaluation across 5 LLMs and 50 queries. Decomposes analytical workflows into four specialized agents (Intent, Discovery, Analysis, Report) coordinated by orchestrator. Key distinction: Universal agents provide consistent value; Conditional agents vary by model capability.

**Key Method:** Four-agent sequential architecture:
1. Intent Clarifying Agent - resolves query ambiguities
2. Data Discovery Agent - semantic search across repositories (data.gov)
3. Data Analysis Agent - Python code generation with validation
4. Report Generation Agent - synthesis with traceability

Evaluation via LLM-as-Judge protocol measuring Factual Consistency, Completeness, Relevance, Coherence.

**Implementation Check:**
- GitHub repos: No direct PublicAgent implementation; related projects:
  - LAMBDA (https://github.com/AMA-CMFAI/LAMBDA) - code-free multi-agent data analysis
  - DataMind (https://github.com/zjunlp/DataMind) - AAAI 2026 open-source data agents
- Commercial use: No - academic research (University of North Texas)
- Open questions: How do principles generalize beyond data analysis domain? Optimal orchestrator design for agent routing?

**Five Design Principles:**

| Principle | Finding |
|-----------|---------|
| 1. Model-Independent Value | Even strongest model shows 97.5% agent win rates |
| 2. Universal vs Conditional | Universal agents (Discovery, Analysis): 12.4% std dev; Conditional (Intent, Report): 20.5% std dev |
| 3. Non-Redundant Specialization | Removing Discovery/Analysis = catastrophic (243-280 failures); Intent/Report = quality degradation |
| 4. Complexity-Independent Benefits | Win rates stable across difficulty: 84-94% |
| 5. Model-Agent Fit Dependency | Analysis agent: 42-96% win rate variance across models |

**Model Comparison:**

| Model | Score |
|-------|-------|
| GPT OSS 120B | 8.2/10 |
| Gemini 2.5 Pro | 7.2/10 |
| GPT-4o Mini | 6.8/10 |
| Grok 3 Mini | 5.8/10 |
| Llama 3.3 70B | 4.7/10 |

**Score Breakdown:**
- Novelty: 4/5 (Five empirically-validated principles; Universal/Conditional taxonomy)
- Feasibility: 4/5 (Clear agent roles; requires orchestrator)
- Time-to-POC: 4/5 (Architecture well-defined; deployment guidance provided)
- Value/Market: 4/5 (Open data accessibility real need; principles generalizable)
- Defensibility: 2/5 (Principles publicly documented)
- Adoption: 4/5 (Framework-agnostic principles)

**Decision Rationale:** Strong empirical validation distinguishes from theoretical pattern papers. Universal vs Conditional taxonomy provides concrete deployment guidance. Model profiling requirement (20-50 queries) is practical production advice.

**Extracted Insights:**
- Universal vs Conditional agent taxonomy enables principled deployment decisions
- Model size poorly predicts performance (120B: 8.2/10 vs 70B: 4.7/10) - architecture matters more
- Catastrophic vs degradation failure modes guide agent prioritization
- Profile 20-50 queries before production to assess model-agent fit

**Cross-References:**
- Agent specialization relates to SIE pattern from arxiv_2601.03328
- Failure mode analysis complements resilience patterns from arxiv_2509.08646
- Framework selection heuristics from arxiv_2508.10146 apply to agent deployment
- Four-agent architecture parallels four-component model from arxiv_2510.09244

**Learnings for Future Iterations:**
- Empirical validation across multiple models strengthens design principle claims
- LLM-as-Judge methodology increasingly common for agent evaluation
- Model profiling before deployment is emerging production best practice
- Lead author (Montazeri) has industry background - principles likely grounded in practical experience

---

## 2026-01-14 - Paper: MAR: Multi-Agent Reflexion Improves Reasoning Abilities in LLMs
ID: arxiv_2512.20845
Status: PRESENTED
Score: 20/30

**Summary:** Addresses degeneration-of-thought in single-LLM Reflexion by replacing self-critique with structured multi-agent debate among persona-based critics. Multiple agents with distinct roles analyze failures from different perspectives before a Judge synthesizes consensus reflection. Demonstrates that diverse viewpoints break repetitive error cycles inherent in single-agent self-reflection.

**Key Method:** Six-step pipeline:
1. Actor Attempt - LLM generates initial response
2. Evaluation - System checks correctness
3. Initial Diagnosis - Each persona analyzes failure reasons independently
4. Debates - Up to 2 rounds of agreement/disagreement between personas
5. Consensus Reflection - Judge synthesizes debate into actionable guidance
6. Retry - Actor attempts task again with reflection in memory

**Personas Defined:**

| Domain | Personas |
|--------|----------|
| **Reasoning (HotPotQA)** | Verifier (high exploit), Skeptic (high explore, counters confirmation bias), Logician (strict specification), Creative (divergent thinking), Meta-Reflector (process analysis) |
| **Programming (HumanEval)** | Senior Engineer (correctness/efficiency), QA Engineer (edge cases), Algorithm Expert (algorithmic correctness), Code Reviewer (bugs/style) |

**Implementation Check:**
- GitHub repos: Yes (related multi-agent debate frameworks)
  - https://github.com/Skytliang/Multi-Agents-Debate (MAD framework)
  - https://github.com/composable-models/llm_multiagent_debate (ICML 2024)
  - https://github.com/instadeepai/DebateLLM (benchmarking library)
  - AutoGen native support: https://microsoft.github.io/autogen/stable//user-guide/core-user-guide/design-patterns/multi-agent-debate.html
- Commercial use: No direct commercialization; AutoGen (Microsoft) provides built-in pattern
- Open questions: Optimal number of debate rounds? How to minimize 3x cost overhead? Domain-specific persona transfer?

**Benchmark Results:**

| Benchmark | Baseline | Reflexion | MAR | Improvement |
|-----------|----------|-----------|-----|-------------|
| HotPotQA EM | 32.0% | 44.0% | 47.0% | +3 pts |
| HumanEval pass@1 | 67.1% | 76.4% | 82.6% | +6.2 pts |

**Score Breakdown:**
- Novelty: 4/5 (Principled solution to documented Reflexion failure; persona design systematic)
- Feasibility: 4/5 (Clear architecture; builds on existing Reflexion framework)
- Time-to-POC: 3/5 (3x API cost increase; requires persona engineering per domain)
- Value/Market: 3/5 (Benchmark improvements modest; cost increase significant barrier)
- Defensibility: 2/5 (Persona definitions public; builds on existing multi-agent debate literature)
- Adoption: 4/5 (AutoGen native support; works with existing frameworks)

**Decision Rationale:** Meets threshold (20/30). Extends Reflexion with principled solution to degeneration-of-thought. Persona design template valuable for domain adaptation. AutoGen native support simplifies adoption. Key trade-off: 3x cost for 3-6 point improvement.

**Extracted Insights:**
- Degeneration-of-thought: Single LLM repeats errors despite explicit failure feedback due to confirmation bias and mode collapse
- Multi-persona debate breaks error cycles by introducing diverse reasoning perspectives
- Persona engineering varies on exploit/explore/strictness axes for reasoning; mirrors real-world roles for programming
- Cost-benefit: ~300-400 API calls per task (3x Reflexion) for marginal gains - evaluate per use case

**Documented Failure Modes Addressed:**
- Hallucinated Specification Drift - Reflexion invents new task spec and steers away from true objective
- Confirmation Bias - Single agent repeats initial errors across iterations
- Mode Collapse - Identical buggy structures reproduced despite feedback

**Cross-References:**
- Directly extends Reflexion pattern from arxiv_2505.07087 (cognitive patterns)
- Multi-agent debate relates to collaboration patterns from arxiv_2404.11584
- Persona specialization aligns with Universal/Conditional taxonomy from arxiv_2511.03023
- Cost-benefit trade-off mirrors P-t-E latency discussion from arxiv_2509.08646

**Learnings for Future Iterations:**
- Multi-agent debate is increasingly mainstream (ICML 2024, AutoGen native support)
- Persona design is domain-specific - requires explicit engineering for new applications
- 3x cost increase may limit production adoption despite benchmark improvements
- EM metric limitations (penalizes correct answers for formatting) suggest benchmark skepticism warranted

---

## 2026-01-14 - Paper: The Lighthouse of Language: Enhancing LLM Agents via Critique-Guided Improvement
ID: arxiv_2503.16024
Status: PRESENTED
Score: 21/30

**Summary:** Two-player actor-critic framework for LLM agents where a critic model generates structured natural language feedback (grades + revision suggestions) and an actor model learns to incorporate critiques through iterative supervised fine-tuning. Key finding: small Llama-3-8B critic substantially outperforms GPT-4 (+29.16% on feedback quality), demonstrating task-specific fine-tuned critics beat general-purpose models.

**Key Method:** Critique-Guided Improvement (CGI) operates in two stages:
1. **Critique Generation:** Critic evaluates actions across three dimensions (task contribution, feasibility, efficiency) and produces structured assessments with grades (Excellent/Good/Neutral/Poor/Very Poor) plus actionable revision suggestions
2. **Action Refinement:** Actor learns to leverage critiques through iterative SFT with exploration-learning cycles

Addresses 'policy misalignment' by matching critiques to actor's current policy level - critiques designed for experts fail when applied to novices.

**Implementation Check:**
- GitHub repos: Placeholder only
  - https://github.com/rhyang2021/CGI (placeholder - no code released yet)
  - https://github.com/drdh/LAC (related LLM Actor-Critic framework with code)
- Commercial use: No direct commercialization
  - Authors from Fudan University and Tencent Hunyuan
  - Tencent has commercial agent products (QBot, Yuanbao with 41.64M MAU) but CGI pattern not directly productized
- Open questions: When will official implementation be released? How to reduce 4x cost overhead? Minimal training data requirements?

**Benchmark Results:**

| Environment | No Critique | GPT-4o | CGI (3 iterations) | CGI Gain |
|-------------|-------------|--------|-------------------|----------|
| WebShop | 13.49% | 17.78% | 76.17% | +326% |
| ScienceWorld | 14.48% | 33.06% | 78.43% | +137% |
| TextCraft | 10.00% | 46.00% | 68.00% | +48% |
| **Average** | 12.65% | 32.28% | **74.20%** | +130% |

Training: 32K expert-annotated examples (14K ScienceWorld, 10K WebShop, 8K TextCraft) on 8 A100 GPUs for 3 epochs.

**Score Breakdown:**
- Novelty: 4/5 (Actor-critic with NL feedback novel; small model beating GPT-4 is significant contribution)
- Feasibility: 4/5 (Clear methodology; standard fine-tuning approach; 8 A100s accessible)
- Time-to-POC: 3/5 (No working implementation yet; ~32K training examples required; GPT-4o needed for annotation)
- Value/Market: 4/5 (NeurIPS 2025 acceptance validates quality; Tencent affiliation suggests enterprise applicability)
- Defensibility: 2/5 (Technique builds on existing actor-critic; critic training approach is documented)
- Adoption: 4/5 (Works with open-source models; 4x cost increase manageable; framework-agnostic)

**Decision Rationale:** NeurIPS 2025 accepted paper with strong empirical results. Novel contribution: task-specific fine-tuned critic substantially outperforms GPT-4 on feedback quality. Two-player framework separates evaluation from action, enabling independent optimization. 4x cost overhead is significant but 130%+ performance gains justify for high-value tasks.

**Extracted Insights:**
- Task-specific fine-tuned critics beat general-purpose models: 8B critic > GPT-4o on evaluation tasks
- Critique-guided architecture decouples evaluation from action - each component independently optimizable
- Policy misalignment: critics must match actor's current policy level; iterative co-training addresses this
- Early exploration benefit: highest revision frequency in stage 1, critiques primarily guide initial search

**Cross-References:**
- Extends Reflexion pattern from arxiv_2505.07087 with structured external feedback
- Critique approach complements multi-agent debate from arxiv_2512.20845 (both address self-reflection limitations)
- Iterative refinement relates to step-wise reflection pattern (arxiv_2505.07087)
- Specialist beats generalist finding aligns with Universal agent taxonomy from arxiv_2511.03023

**Learnings for Future Iterations:**
- NeurIPS/top-tier venue acceptance strongly validates research quality
- Tencent affiliation suggests potential for enterprise adoption pathway
- Placeholder GitHub repos common for recent papers - check back later for code releases
- 4x cost overhead is emerging theme in improved feedback patterns (cf. 3x for MAR arxiv_2512.20845)

---

## 2026-01-14 - Paper: Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to Deliberation
ID: arxiv_2511.02303
Status: PRESENTED
Score: 22/30

**Summary:** Identifies and solves the 'lazy agent' problem in multi-agent LLM reasoning where one agent dominates while the other contributes minimally, effectively collapsing the multi-agent setup to single-agent behavior. Root cause traced mathematically to GRPO loss normalization (1/T_i) which inadvertently favors shorter trajectories. Proposes Dr. MAMR (Multi-Agent Meta-Reasoning Done Right) with Shapley-style causal influence measurement and verifiable restart mechanism.

**Key Method:** Two-agent architecture: meta-thinking agent proposes plans and monitors progress; reasoning agent executes through sequential conversational turns. Three key innovations:
1. **Causal Influence Measurement:** Shapley-inspired approach grouping semantically similar steps across rollouts (0.9 cosine threshold) to compute trajectory-independent contribution estimates via probability differences between full/masked histories
2. **Verifiable Restart Mechanism:** `<restart>` control token allowing reasoning agent to discard noisy outputs when masking earlier reasoning increases correct answer confidence (+1 reward), decreases it (-1), or no effect (0)
3. **Step-level Advantage:** Combines normalized outcome rewards, causal influence scores, and restart signals (α=β=0.1)

**Implementation Check:**
- GitHub repos: No direct implementation released
  - Related: https://github.com/kyegomez/awesome-multi-agent-papers (tracks paper)
  - Related: https://github.com/TsinghuaC3I/MARTI (MARL training framework)
  - Related: https://github.com/LazyAGI/LazyLLM (low-code multi-agent)
- Commercial use: No - academic research
- Venue: ICLR 2026 submission (under review, Submission #14685)
- Open questions: When will code be released? How do results generalize beyond mathematical reasoning?

**Benchmark Results (7B Qwen2.5):**

| Benchmark | Dr. MAMR | ReMA | Single GRPO | Improvement |
|-----------|----------|------|-------------|-------------|
| MATH500 | 78.6% | 74.4% | - | +4.2 pts |
| AIME24 | 20.0% | 13.33% | - | +6.67 pts |
| AMC23 | 62.5% | 50.0% | - | +12.5 pts |
| **Average (7 benchmarks)** | **58.43%** | 51.97% | 55.08% | +6.46 pts |

**Score Breakdown:**
- Novelty: 5/5 (First theoretical explanation of lazy agent behavior via GRPO loss structure)
- Feasibility: 4/5 (Built on Verl RL framework; requires RL training expertise and GPU resources)
- Time-to-POC: 3/5 (No public implementation; requires SFT cold-start + RL training)
- Value/Market: 4/5 (ICLR 2026 submission; addresses fundamental multi-agent training failure)
- Defensibility: 3/5 (Novel theorem on GRPO bias; causal influence principled approach)
- Adoption: 3/5 (Requires RL infrastructure; domain-specific to math reasoning)

**Decision Rationale:** ICLR 2026 submission addressing fundamental multi-agent collaboration failure mode. First paper to theoretically explain why lazy agents emerge through loss function analysis (Theorem 1). Key insight: normalization favoring shorter trajectories drives agents to minimize collaboration. Restart mechanism enables error recovery in multi-turn contexts - directly addresses accumulated error problem. Score 22/30 meets threshold.

**Extracted Insights:**
- Lazy agent problem root cause: GRPO loss normalization (1/T_i) inadvertently favors shorter trajectories, implicitly incentivizing agents to minimize turns
- Shapley-style causal influence overcomes phrasing bias by grouping semantically similar steps across rollouts
- Verifiable restart mechanism critical for multi-turn contexts where accumulated errors compound
- Training stability: Dr. MAMR maintains stable training while ReMA collapsed after 150 steps

**Cross-References:**
- Extends multi-agent collaboration analysis from arxiv_2404.11584 (identifies when multi-agent fails)
- Relates to degeneration-of-thought problem in arxiv_2512.20845 (different mechanism, same outcome - single agent dominates)
- Causal influence measurement complements credit assignment approaches in actor-critic (arxiv_2503.16024)
- Restart mechanism relates to commitment & reconsideration pattern from arxiv_2505.07087 (cognitive patterns)

**Learnings for Future Iterations:**
- ICLR/top-tier venue submissions with theoretical contributions (theorems) indicate high research rigor
- Lazy agent problem is a distinct failure mode from degeneration-of-thought - both collapse multi-agent to single-agent but via different mechanisms
- Training stability metrics (collapse at N steps) provide concrete comparison criteria
- Domain-specific results (math reasoning) require careful extrapolation to other agent domains

---
