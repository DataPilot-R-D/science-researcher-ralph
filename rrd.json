{
  "project": "Research: Robotics x LLMs",
  "branchName": "research/robotics-llms",
  "description": "Scout recent work on robotics combined with LLMs and language-conditioned control. Focus on VLA models, instruction-following robot control, tool use, planning, navigation, and sim2real transfer with practical implementation potential. Scoring rubric: 0-5 each for novelty, feasibility, time-to-POC, value/market, defensibility, and adoption (total 0-30); PRESENT if score >= 18.",
  "requirements": {
    "focus_area": "robotics / embodied AI + LLMs",
    "keywords": [
      "language-conditioned policy",
      "vision-language-action",
      "VLA model",
      "instruction-following robot control",
      "robot tool use",
      "LLM planning",
      "task and motion planning",
      "semantic navigation",
      "embodied LLM",
      "robot foundation model",
      "sim2real transfer",
      "robot manipulation",
      "multimodal policy",
      "affordance grounding"
    ],
    "time_window_days": 365,
    "target_papers": 20,
    "sources": [
      "arXiv",
      "Google Scholar",
      "web"
    ],
    "min_score_to_present": 18
  },
  "phase": "ANALYSIS",
  "papers_pool": [
    {
      "id": "arxiv_2601.06652",
      "title": "Follow the Signs: Using Textual Cues and LLMs to Guide Efficient Robot Navigation",
      "url": "http://arxiv.org/abs/2601.06652v1",
      "pdf_url": "https://arxiv.org/pdf/2601.06652v1",
      "authors": [
        "Jing Cao",
        "Nishanth Kumar",
        "Aidan Curtis"
      ],
      "date": "2026-01-10",
      "source": "arXiv",
      "priority": 5,
      "status": "presented",
      "score": 19,
      "score_breakdown": {
        "novelty": 3,
        "feasibility": 4,
        "time_to_poc": 4,
        "value_market": 3,
        "defensibility": 2,
        "adoption": 3
      },
      "analysis": {
        "summary": "Proposes an LLM-guided semantic navigation framework that uses textual cues (room numbers, signs) to predict likely goal regions in structured indoor maps, then combines these predictions with frontier exploration and A* to reach goals more efficiently under partial observability.",
        "methodology": "Maintain seen occupancy/semantic grids; prompt an LLM to infer numbering or signage patterns and output a relative goal region. Update a confidence grid with exponential decay and zero out explored non-goal cells; plan with A* to the highest-confidence region, otherwise fall back to nearest frontier exploration.",
        "results": "Across seven grid environments (synthetic, real floor plans, and a noisy Polycam scan), the method achieves higher SPL and success rates than NavGPT, LLM-only, and frontier-only baselines; overall SPL 0.745 vs 0.596 for frontier and 0.236 for NavGPT, with a real-world Spot robot demo navigating to a room using signage cues.",
        "implementations_found": [],
        "commercialized": false,
        "limitations": "Requires pre-annotated semantic grids and assumes full kxk local observability; performance degrades in noisy maps and when numbering patterns are ambiguous or inconsistent."
      },
      "decision": "PRESENT",
      "notes": "LLM reasoning over signage patterns plus confidence-grid guidance improves navigation efficiency; limitations around perception and ambiguous numbering remain."
    },
    {
      "id": "arxiv_2601.06552",
      "title": "Model Reconciliation through Explainability and Collaborative Recovery in Assistive Robotics",
      "url": "http://arxiv.org/abs/2601.06552v1",
      "pdf_url": "https://arxiv.org/pdf/2601.06552v1",
      "authors": [
        "Britt Besch",
        "Tai Mai",
        "Jeremias Thun",
        "Markus Huff",
        "J\u00f6rn Vogel",
        "Freek Stulp",
        "Samuel Bustamante"
      ],
      "date": "2026-01-10",
      "source": "arXiv",
      "priority": 5,
      "status": "presented",
      "score": 18,
      "score_breakdown": {
        "novelty": 3,
        "feasibility": 3,
        "time_to_poc": 4,
        "value_market": 3,
        "defensibility": 2,
        "adoption": 3
      },
      "analysis": {
        "summary": "Introduces an LLM-driven model reconciliation framework for shared-control assistive robots that predicts and explains differences between human and robot mental models without requiring an explicit user model, then supports collaborative recovery through user rebuttals.",
        "methodology": "Robot knowledge is split into a static object database, a world model of object instances, and an action model (PDDL-based action graph). An LLM extracts objects/actions from a user query and matches them against these models in four steps to classify divergence types (D_GO, D_SO, D_GA, D_SA, or FD) and generate explanations. For recovery, a VLM matches rebuttal objects in the camera view to suggest robot repositioning or update the world model; unmet preconditions are resolved by translating user rebuttals into symbolic state updates.",
        "results": "Pilot tests on a real wheelchair-based mobile manipulator showed correct explanations and successful recovery in daily living scenarios. In a digital-twin evaluation (40 episodes, 120 runs), the method achieved 100% accuracy on object localization explanations, 78.79% on symbolic state explanations, and 78.12% on recovery suggestions; adding a translation dictionary for uncommon terms improved precondition explanation accuracy from 78.8% to 92.4% (Cohen\u2019s kappa 0.91 between human and LLM labels).",
        "implementations_found": [],
        "commercialized": false,
        "limitations": "Recovery movement suggestions are weaker due to VLM spatial reasoning errors; adjectives can confuse smaller LLMs; evaluation covers a limited set of daily living scenarios and relies on semantically interpretable robot labels."
      },
      "decision": "PRESENT",
      "notes": "Practical LLM-based model reconciliation with real-robot validation; useful for explainability and collaborative recovery in assistive robotics, though dependent on clean semantic labels and limited scenarios."
    },
    {
      "id": "arxiv_2601.05529",
      "title": "Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making",
      "url": "http://arxiv.org/abs/2601.05529v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05529v1",
      "authors": [
        "Jua Han",
        "Jaeyoon Seo",
        "Jungbin Min",
        "Jean Oh",
        "Jihie Kim"
      ],
      "date": "2026-01-09",
      "source": "arXiv",
      "priority": 5,
      "status": "presented",
      "score": 19,
      "score_breakdown": {
        "novelty": 3,
        "feasibility": 4,
        "time_to_poc": 4,
        "value_market": 3,
        "defensibility": 2,
        "adoption": 3
      },
      "analysis": {
        "summary": "First systematic safety benchmark for LLM-controlled robots, introducing seven tasks across three categories (Complete Information, Incomplete Information, Safety-Oriented Spatial Reasoning) to evaluate failure modes in safety-critical scenarios like fire evacuations.",
        "methodology": "Complete Information tasks use ASCII maps to isolate spatial reasoning; Incomplete Information tasks test hallucinations via sequence reasoning and uncertain terrain; SOSR tasks evaluate direction sense and emergency decision-making. Tested GPT-5, GPT-4o, Gemini-2.5 Flash, Gemini-2.0 Flash, and LLaMA-3-8b.",
        "results": "LLaMA-3-8b achieved 0% on ASCII navigation; Gemini-2.5 Flash directed users to professor's office (32%) or hallucinated server room (1%) instead of emergency exits in fire scenarios; GPT-5 achieved 100% on hard deterministic tasks but even best models show catastrophic failures in edge cases.",
        "implementations_found": [],
        "commercialized": false,
        "limitations": "Hardware constraints limited evaluation to smaller models; preliminary dataset (100 video sequences); no concrete mitigation strategies proposed; authors acknowledge this is a mini-benchmark requiring expansion."
      },
      "decision": "PRESENT",
      "notes": "Critical diagnostic for LLM-robotics: even 99% accuracy is dangerous when 1/100 executions could cause catastrophic harm. No code released yet."
    },
    {
      "id": "arxiv_2601.03590",
      "title": "Can LLMs See Without Pixels? Benchmarking Spatial Intelligence from Textual Descriptions",
      "url": "http://arxiv.org/abs/2601.03590v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03590v1",
      "authors": [
        "Zhongbin Guo",
        "Zhen Yang",
        "Yushan Li",
        "Xinyue Zhang",
        "Wenyu Gao",
        "Jiacheng Wang",
        "Chengzhi Li",
        "Xiangrui Liu",
        "Ping Jian"
      ],
      "date": "2026-01-07",
      "source": "arXiv",
      "priority": 5,
      "status": "pending",
      "score": null,
      "score_breakdown": null,
      "analysis": null,
      "decision": null,
      "notes": ""
    },
    {
      "id": "arxiv_2601.02456",
      "title": "InternVLA-A1: Unifying Understanding, Generation and Action for Robotic Manipulation",
      "url": "http://arxiv.org/abs/2601.02456v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02456v1",
      "authors": [
        "Junhao Cai",
        "Zetao Cai",
        "Jiafei Cao",
        "Yilun Chen",
        "Zeyu He",
        "Lei Jiang",
        "Hang Li",
        "Hengjie Li",
        "Yang Li",
        "Yufei Liu",
        "Yanan Lu",
        "Qi Lv",
        "Haoxiang Ma",
        "Jiangmiao Pang",
        "Yu Qiao",
        "Zherui Qiu",
        "Yanqing Shen",
        "Xu Shi",
        "Yang Tian",
        "Bolun Wang",
        "Hanqing Wang",
        "Jiaheng Wang",
        "Tai Wang",
        "Xueyuan Wei",
        "Chao Wu",
        "Yiman Xie",
        "Boyang Xing",
        "Yuqiang Yang",
        "Yuyin Yang",
        "Qiaojun Yu",
        "Feng Yuan",
        "Jia Zeng",
        "Jingjing Zhang",
        "Shenghan Zhang",
        "Shi Zhang",
        "Zhuoma Zhaxi",
        "Bowen Zhou",
        "Yuanzhen Zhou",
        "Yunsong Zhou",
        "Hongrui Zhu",
        "Yangkun Zhu",
        "Yuchen Zhu"
      ],
      "date": "2026-01-05",
      "source": "arXiv",
      "priority": 5,
      "status": "pending",
      "score": null,
      "score_breakdown": null,
      "analysis": null,
      "decision": null,
      "notes": ""
    },
    {
      "id": "arxiv_2601.02078",
      "title": "Genie Sim 3.0 : A High-Fidelity Comprehensive Simulation Platform for Humanoid Robot",
      "url": "http://arxiv.org/abs/2601.02078v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02078v1",
      "authors": [
        "Chenghao Yin",
        "Da Huang",
        "Di Yang",
        "Jichao Wang",
        "Nanshu Zhao",
        "Chen Xu",
        "Wenjun Sun",
        "Linjie Hou",
        "Zhijun Li",
        "Junhui Wu",
        "Zhaobo Liu",
        "Zhen Xiao",
        "Sheng Zhang",
        "Lei Bao",
        "Rui Feng",
        "Zhenquan Pang",
        "Jiayu Li",
        "Qian Wang",
        "Maoqing Yao"
      ],
      "date": "2026-01-05",
      "source": "arXiv",
      "priority": 5,
      "status": "pending",
      "score": null,
      "score_breakdown": null,
      "analysis": null,
      "decision": null,
      "notes": ""
    },
    {
      "id": "arxiv_2601.01321",
      "title": "Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models",
      "url": "http://arxiv.org/abs/2601.01321v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01321v1",
      "authors": [
        "Rong Zhou",
        "Dongping Chen",
        "Zihan Jia",
        "Yao Su",
        "Yixin Liu",
        "Yiwen Lu",
        "Dongwei Shi",
        "Yue Huang",
        "Tianyang Xu",
        "Yi Pan",
        "Xinliang Li",
        "Yohannes Abate",
        "Qingyu Chen",
        "Zhengzhong Tu",
        "Yu Yang",
        "Yu Zhang",
        "Qingsong Wen",
        "Gengchen Mai",
        "Sunyang Fu",
        "Jiachen Li",
        "Xuyu Wang",
        "Ziran Wang",
        "Jing Huang",
        "Tianming Liu",
        "Yong Chen",
        "Lichao Sun",
        "Lifang He"
      ],
      "date": "2026-01-04",
      "source": "arXiv",
      "priority": 5,
      "status": "pending",
      "score": null,
      "score_breakdown": null,
      "analysis": null,
      "decision": null,
      "notes": ""
    },
    {
      "id": "arxiv_2601.01196",
      "title": "EduSim-LLM: An Educational Platform Integrating Large Language Models and Robotic Simulation for Beginners",
      "url": "http://arxiv.org/abs/2601.01196v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01196v1",
      "authors": [
        "Shenqi Lu",
        "Liangwei Zhang"
      ],
      "date": "2026-01-03",
      "source": "arXiv",
      "priority": 5,
      "status": "pending",
      "score": null,
      "score_breakdown": null,
      "analysis": null,
      "decision": null,
      "notes": ""
    },
    {
      "id": "arxiv_2512.22615",
      "title": "Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone",
      "url": "http://arxiv.org/abs/2512.22615v2",
      "pdf_url": "https://arxiv.org/pdf/2512.22615v2",
      "authors": [
        "Jiacheng Ye",
        "Shansan Gong",
        "Jiahui Gao",
        "Junming Fan",
        "Shuang Wu",
        "Wei Bi",
        "Haoli Bai",
        "Lifeng Shang",
        "Lingpeng Kong"
      ],
      "date": "2025-12-27",
      "source": "arXiv",
      "priority": 5,
      "status": "pending",
      "score": null,
      "score_breakdown": null,
      "analysis": null,
      "decision": null,
      "notes": ""
    },
    {
      "id": "arxiv_2512.22414",
      "title": "Emergence of Human to Robot Transfer in Vision-Language-Action Models",
      "url": "http://arxiv.org/abs/2512.22414v1",
      "pdf_url": "https://arxiv.org/pdf/2512.22414v1",
      "authors": [
        "Simar Kareer",
        "Karl Pertsch",
        "James Darpinian",
        "Judy Hoffman",
        "Danfei Xu",
        "Sergey Levine",
        "Chelsea Finn",
        "Suraj Nair"
      ],
      "date": "2025-12-27",
      "source": "arXiv",
      "priority": 5,
      "status": "pending",
      "score": null,
      "score_breakdown": null,
      "analysis": null,
      "decision": null,
      "notes": ""
    },
    {
      "id": "arxiv_2512.21859",
      "title": "TimeBill: Time-Budgeted Inference for Large Language Models",
      "url": "http://arxiv.org/abs/2512.21859v1",
      "pdf_url": "https://arxiv.org/pdf/2512.21859v1",
      "authors": [
        "Qi Fan",
        "An Zou",
        "Yehan Ma"
      ],
      "date": "2025-12-26",
      "source": "arXiv",
      "priority": 5,
      "status": "pending",
      "score": null,
      "score_breakdown": null,
      "analysis": null,
      "decision": null,
      "notes": ""
    },
    {
      "id": "arxiv_2512.23739",
      "title": "Break Out the Silverware -- Semantic Understanding of Stored Household Items",
      "url": "http://arxiv.org/abs/2512.23739v1",
      "pdf_url": "https://arxiv.org/pdf/2512.23739v1",
      "authors": [
        "Michaela Levi-Richter",
        "Reuth Mirsky",
        "Oren Glickman"
      ],
      "date": "2025-12-25",
      "source": "arXiv",
      "priority": 5,
      "status": "pending",
      "score": null,
      "score_breakdown": null,
      "analysis": null,
      "decision": null,
      "notes": ""
    },
    {
      "id": "arxiv_2512.21293",
      "title": "Quadrupped-Legged Robot Movement Plan Generation using Large Language Model",
      "url": "http://arxiv.org/abs/2512.21293v1",
      "pdf_url": "https://arxiv.org/pdf/2512.21293v1",
      "authors": [
        " Muhtadin",
        "Vincentius Gusti Putu A. B. M.",
        "Ahmad Zaini",
        "Mauridhi Hery Purnomo",
        "I Ketut Eddy Purnama",
        "Chastine Fatichah"
      ],
      "date": "2025-12-24",
      "source": "arXiv",
      "priority": 5,
      "status": "pending",
      "score": null,
      "score_breakdown": null,
      "analysis": null,
      "decision": null,
      "notes": ""
    },
    {
      "id": "arxiv_2512.21243",
      "title": "LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation",
      "url": "http://arxiv.org/abs/2512.21243v1",
      "pdf_url": "https://arxiv.org/pdf/2512.21243v1",
      "authors": [
        "Anatoly O. Onishchenko",
        "Alexey K. Kovalev",
        "Aleksandr I. Panov"
      ],
      "date": "2025-12-24",
      "source": "arXiv",
      "priority": 5,
      "status": "pending",
      "score": null,
      "score_breakdown": null,
      "analysis": null,
      "decision": null,
      "notes": ""
    },
    {
      "id": "arxiv_2512.20206",
      "title": "TongSIM: A General Platform for Simulating Intelligent Machines",
      "url": "http://arxiv.org/abs/2512.20206v1",
      "pdf_url": "https://arxiv.org/pdf/2512.20206v1",
      "authors": [
        "Zhe Sun",
        "Kunlun Wu",
        "Chuanjian Fu",
        "Zeming Song",
        "Langyong Shi",
        "Zihe Xue",
        "Bohan Jing",
        "Ying Yang",
        "Xiaomeng Gao",
        "Aijia Li",
        "Tianyu Guo",
        "Huiying Li",
        "Xueyuan Yang",
        "Rongkai Liu",
        "Xinyi He",
        "Yuxi Wang",
        "Yue Li",
        "Mingyuan Liu",
        "Yujie Lu",
        "Hongzhao Xie",
        "Shiyun Zhao",
        "Bo Dai",
        "Wei Wang",
        "Tao Yuan",
        "Song-Chun Zhu",
        "Yujia Peng",
        "Zhenliang Zhang"
      ],
      "date": "2025-12-23",
      "source": "arXiv",
      "priority": 5,
      "status": "pending",
      "score": null,
      "score_breakdown": null,
      "analysis": null,
      "decision": null,
      "notes": ""
    },
    {
      "id": "arxiv_2512.17992",
      "title": "Unifying Deep Predicate Invention with Pre-trained Foundation Models",
      "url": "http://arxiv.org/abs/2512.17992v1",
      "pdf_url": "https://arxiv.org/pdf/2512.17992v1",
      "authors": [
        "Qianwei Wang",
        "Bowen Li",
        "Zhanpeng Luo",
        "Yifan Xu",
        "Alexander Gray",
        "Tom Silver",
        "Sebastian Scherer",
        "Katia Sycara",
        "Yaqi Xie"
      ],
      "date": "2025-12-19",
      "source": "arXiv",
      "priority": 5,
      "status": "pending",
      "score": null,
      "score_breakdown": null,
      "analysis": null,
      "decision": null,
      "notes": ""
    },
    {
      "id": "arxiv_2512.17435",
      "title": "ImagineNav++: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination",
      "url": "http://arxiv.org/abs/2512.17435v2",
      "pdf_url": "https://arxiv.org/pdf/2512.17435v2",
      "authors": [
        "Teng Wang",
        "Xinxin Zhao",
        "Wenzhe Cai",
        "Changyin Sun"
      ],
      "date": "2025-12-19",
      "source": "arXiv",
      "priority": 5,
      "status": "pending",
      "score": null,
      "score_breakdown": null,
      "analysis": null,
      "decision": null,
      "notes": ""
    },
    {
      "id": "arxiv_2512.17309",
      "title": "RecipeMasterLLM: Revisiting RoboEarth in the Era of Large Language Models",
      "url": "http://arxiv.org/abs/2512.17309v1",
      "pdf_url": "https://arxiv.org/pdf/2512.17309v1",
      "authors": [
        "Asil Kaan Bozcuoglu",
        "Ziyuan Liu"
      ],
      "date": "2025-12-19",
      "source": "arXiv",
      "priority": 5,
      "status": "pending",
      "score": null,
      "score_breakdown": null,
      "analysis": null,
      "decision": null,
      "notes": ""
    },
    {
      "id": "arxiv_2512.17183",
      "title": "Semantic Co-Speech Gesture Synthesis and Real-Time Control for Humanoid Robots",
      "url": "http://arxiv.org/abs/2512.17183v1",
      "pdf_url": "https://arxiv.org/pdf/2512.17183v1",
      "authors": [
        "Gang Zhang"
      ],
      "date": "2025-12-19",
      "source": "arXiv",
      "priority": 5,
      "status": "pending",
      "score": null,
      "score_breakdown": null,
      "analysis": null,
      "decision": null,
      "notes": ""
    },
    {
      "id": "arxiv_2601.05248",
      "title": "LaST$_{0}$: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model",
      "url": "http://arxiv.org/abs/2601.05248v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05248v1",
      "authors": [
        "Zhuoyang Liu",
        "Jiaming Liu",
        "Hao Chen",
        "Ziyu Guo",
        "Chengkai Hou",
        "Chenyang Gu",
        "Jiale Yu",
        "Xiangju Mi",
        "Renrui Zhang",
        "Zhengping Che",
        "Jian Tang",
        "Pheng-Ann Heng",
        "Shanghang Zhang"
      ],
      "date": "2026-01-08",
      "source": "arXiv",
      "priority": 5,
      "status": "pending",
      "score": null,
      "score_breakdown": null,
      "analysis": null,
      "decision": null,
      "notes": ""
    }
  ],
  "insights": [
    {
      "id": "insight_001",
      "paper_id": "arxiv_2601.06652",
      "insight": "Directional predictions from LLMs can be stabilized by a decayed confidence grid and combined with frontier exploration to preserve completeness while exploiting textual cues.",
      "tags": [
        "navigation",
        "llm-guidance",
        "frontier-exploration",
        "textual-cues"
      ],
      "cross_refs": []
    },
    {
      "id": "insight_002",
      "paper_id": "arxiv_2601.06552",
      "insight": "LLM-based reconciliation that grounds explanations in robot world/action models can outperform vision-only baselines for failure explanations in assistive robotics.",
      "tags": [
        "explainability",
        "model-reconciliation",
        "assistive-robotics"
      ],
      "cross_refs": []
    },
    {
      "id": "insight_003",
      "paper_id": "arxiv_2601.06552",
      "insight": "Semantic clarity of robot labels matters: adding a translation dictionary for uncommon terms boosted precondition explanation accuracy from 78.8% to 92.4% in their evaluation.",
      "tags": [
        "semantics",
        "prompting",
        "robot-representation"
      ],
      "cross_refs": []
    },
    {
      "id": "insight_004",
      "paper_id": "arxiv_2601.05529",
      "insight": "Even 99% accuracy is dangerous in robotics: 1/100 catastrophic failures is unacceptable for safety-critical systems. Aggregate metrics hide worst-case failures that must be evaluated separately.",
      "tags": [
        "safety",
        "evaluation",
        "metrics"
      ],
      "cross_refs": []
    },
    {
      "id": "insight_005",
      "paper_id": "arxiv_2601.05529",
      "insight": "ASCII map navigation isolates spatial reasoning from visual processing and reveals stark model differences: GPT-5 achieves 100% on hard tasks while LLaMA-3-8b achieves 0%, showing spatial reasoning capability varies dramatically across models.",
      "tags": [
        "spatial-reasoning",
        "benchmark",
        "model-comparison"
      ],
      "cross_refs": []
    }
  ],
  "visited_urls": [
    "https://export.arxiv.org/api/query?search_query=all:vision-language-action&sortBy=submittedDate&sortOrder=descending&max_results=50",
    "https://export.arxiv.org/api/query?search_query=all%3Arobot%20AND%20all%3A%22large%20language%20model%22&sortBy=submittedDate&sortOrder=descending&max_results=50",
    "https://r.jina.ai/http://scholar.google.com/scholar?q=vision+language+action+robotics+2025+arXiv",
    "https://export.arxiv.org/api/query?search_query=ti%3A%22Latent%20Spatio-Temporal%20Chain-of-Thought%22%20AND%20all%3Arobot&sortBy=submittedDate&sortOrder=descending&max_results=5",
    "https://arxiv.org/pdf/2601.06652v1",
    "https://api.github.com/search/repositories?q=%22Follow%20the%20Signs%22%20LLM%20robot%20navigation",
    "https://api.github.com/search/repositories?q=2601.06652",
    "https://duckduckgo.com/html/?q=Follow%20the%20Signs%20LLM%20robot%20navigation",
    "https://arxiv.org/pdf/2601.06552v1",
    "https://api.github.com/search/repositories?q=2601.06552",
    "https://api.github.com/search/repositories?q=%22Model%20Reconciliation%20through%20Explainability%22",
    "https://arxiv.org/abs/2601.05529",
    "https://arxiv.org/html/2601.05529",
    "https://api.github.com/search/repositories?q=2601.05529",
    "https://api.github.com/search/repositories?q=%22Safety%20Not%20Found%22%20LLM%20robotics"
  ],
  "blocked_sources": [],
  "statistics": {
    "total_discovered": 20,
    "total_analyzed": 3,
    "total_presented": 3,
    "total_rejected": 0,
    "total_insights_extracted": 0
  }
}
